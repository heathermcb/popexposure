{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introduction\n",
    "\n",
    "The purpose of this notebook, along with 01_data_setup_example.ipynb and 03_demo_explore_results.ipynb, is to provide a tutorial of how you may want to use the pop_exp pacakge functions.\n",
    "\n",
    "Please see 01_data_setup_example.ipynb before you work through this notebook!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Goals\n",
    "\n",
    "To recap, to demo all the functions available in pop_exp, in this notebook we'll\n",
    "do five separate things, which align with the five options available in three functions in the package. \n",
    "\n",
    "1. Find the total number of people residing within 10km of *any* US wildfire \n",
    "disaster in 2016, 2017, and 2018. \n",
    "2. Find the total number of people residing within 10 km of *each* US wildfire\n",
    "disaster in 2016, 2017, and 2018.\n",
    "3. Find the total number of people residing within 10km of *any* US wildfire \n",
    "disaster in 2016, 2017, and 2018 by 2020 ZCTA. \n",
    "4. Find the total number of people residing within 10 km of *each* US wildfire\n",
    "disaster in 2016, 2017, and 2018 by 2020 ZCTA.\n",
    "5. Find the population of all 2020 ZCTAs. \n",
    "\n",
    "In the last notebook, we prepared the wildfire disaster exposure data and ZCTA\n",
    "data to pass to the pop_exp functions so we could complete these computations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's do it\n",
    "\n",
    "We need to import some libraries and also install and import pop_exp. If you haven't installed pop_exp in the environment you're working in now, go ahead and activate that environment, and pip install pop_exp in the terminal. We can then import the functions within pop ex. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start by importing necessary libraries.\n",
    "import pathlib\n",
    "import sys\n",
    "import glob\n",
    "import pandas as pd\n",
    "# Here's pop_exp\n",
    "from pop_exp import pop_ex_helpers as px"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also set some paths to make it easy to access the data we cleaned for \n",
    "this tutorial. \n",
    "\n",
    "To find the number of people affected by one or more wildfire disaster by year 2016-2018, and by ZCTA, we need to get the paths to each of our wildfire files that we made in the data setup notebook.\n",
    "\n",
    "The regular expression below selects all the files in the interim data directory that have 'fire' in the name. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths \n",
    "base_path = pathlib.Path.cwd().parent\n",
    "data_dir = base_path / \"demo_data\"\n",
    "# wf paths regex\n",
    "wildfire_paths = glob.glob(str(data_dir / \"02_interim_data\" / \"*fire*\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need the path to the population raster we're using, and the ZCTA file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GHSL pop raster\n",
    "ghsl_path = data_dir / \"01_raw_data\" / \"GHS_POP_E2020_GLOBE_R2023A_54009_100_V1_0.tif\"\n",
    "\n",
    "# ZCTA path \n",
    "zcta_path = glob.glob(str(data_dir / \"02_interim_data\" / \"*zcta*\"))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now set up to run the five cases we're interested in. \n",
    "\n",
    "Our first goal was:\n",
    "1. Find the total number of people residing within 10km of *any* US wildfire \n",
    "disaster in 2016, 2017, and 2018. \n",
    "\n",
    "To do this, we can run find_number_of_people_affected with the parameter \n",
    "by_unique_hazrad = False. \n",
    "\n",
    "Because we're looping over three years, we'll initialize an empty list first, \n",
    "and then store the results in this list. We're also adding a year variable to \n",
    "the result as we go. In total, this takes around 24 seconds. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_affected_list = []\n",
    "start_year = 2016\n",
    "\n",
    "for i in range(0, 3):\n",
    "    num_affected = px.find_num_people_affected(\n",
    "        path_to_hazards=wildfire_paths[i],\n",
    "        raster_path=ghsl_path,\n",
    "        by_unique_hazard=False # setting by unique hazard to false \n",
    "    )\n",
    "    num_affected['year'] = start_year + i\n",
    "    num_affected_list.append(num_affected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we added a year variable to each output, we can concatonate these dataframes together, and then look at the output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ok, now we'll join those dataframes together. \n",
    "num_affected_df = pd.concat(num_affected_list, axis=0)\n",
    "num_affected_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our output has three columns: ID_hazard, num_people_affected, and year. \n",
    "We added year, but the other two are output from find_num_people_affected.\n",
    "\n",
    "To find the total number of people affected by any wildfire disaster by year, \n",
    "we could group this output dataframe by year and sum. But, we're about to do two more interesting examples, so we'll leave this output as is, and dive into the output from the later examples in the third section of this tutorial. \n",
    "\n",
    "In our output, one thing has changed: our ID_hazard column is not the same as \n",
    "the ID_hazard column that we started with. \n",
    "\n",
    "We wanted to count the number of people residing within 10km of *any* \n",
    "US wildfire disaster. There are some people who live within 10km of two or\n",
    "more wildfire disasters. Because we just wanted the total, we did not want to \n",
    "double count those people. When computing a total, rather than the number of \n",
    "people affected by each unique hazard, find_num_people_affected takes the unary \n",
    "union of any buffered hazards that are overlapping, and finds the total of \n",
    "everyone residing within that area. In the output, the hazard IDs of any \n",
    "overlapping hazards are concatenated. This avoids double counting, while still \n",
    "giving the user as much information about how many people lived near each \n",
    "hazard or group of hazards as possible. \n",
    "\n",
    "We can save the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_affected_df.to_parquet(data_dir / \"03_results\" / \"num_people_affected_by_wildfire.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also wanted to:\n",
    "\n",
    "2. Find the total number of people residing within 10 km of \n",
    "*EACH* US wildfire disaster in 2016, 2017, and 2018. \n",
    "\n",
    "To do this, we also need to use find_num_people_affected, with all the same \n",
    "arguments except for by_unique_hazard. In this case, we set by_unique_hazard to \n",
    "True. This means that we will count the number of people within 10km of each \n",
    "wildfire disaster boundary, regardless of whether two or more exposed areas \n",
    "overlap. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_affected_list_unique_hazard = []\n",
    "start_year = 2016\n",
    "\n",
    "for i in range(0, 3):\n",
    "    num_affected_unique = px.find_num_people_affected(\n",
    "        path_to_hazards=wildfire_paths[i],\n",
    "        raster_path=ghsl_path,\n",
    "        by_unique_hazard=True # setting by unique hazard to true \n",
    "    )\n",
    "    num_affected_unique['year'] = start_year + i\n",
    "    num_affected_list_unique_hazard.append(num_affected_unique)\n",
    "\n",
    "num_affected_unique = pd.concat(num_affected_list_unique_hazard, axis=0)\n",
    "num_affected_unique.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, our output has three columns: ID_hazard, num_people_affected, and year. \n",
    "\n",
    "This time, the ID_hazard column is the same as the one we passed to this \n",
    "function. This time, if people lived within 10 km of one or more fires, they\n",
    "are counted in the total people affected by that fire. This means people may\n",
    "be double counted or triple or more. \n",
    "\n",
    "We can save the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_affected_unique.to_parquet(data_dir / \"03_results\" / \"num_aff_by_unique_wildfire.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There were two more ways we wanted to define exposure. These are analogous to \n",
    "the two quantities we just computed, but this time, we want to know these\n",
    "exposures by ZCTA. \n",
    "\n",
    "3. Find the total number of people residing within 10km of *any* US wildfire \n",
    "disaster in 2016, 2017, and 2018 by 2020 ZCTA. \n",
    "4. Find the total number of people residing within 10 km of *EACH* US wildfire\n",
    "disaster in 2016, 2017, and 2018 by 2020 ZCTA.\n",
    "\n",
    "To do this, we need to run find_number_of_people_affected_by_geo. \n",
    "\n",
    "First, we'll find the total number of people affected by ZCTA \n",
    "(by_unique_hazard = False).\n",
    "\n",
    "This time, because we need to read in the large ZCTA file, the run will take slightly longer. It should take about 4 minutes, with the majority of this run time being the time it takes to read in the ZCTA file. If you're running these functions on giant datasets, we recommend parallelizeing over space or time. Using parquet files instead of GEOJSON will also speed up this process, but if you're writing geodataframes out of R, geoparquet packages are not fully developed and integrated into R yet and writing parquets can cause errors, so be careful and don't write parquets from R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_affected_zcta_list = []\n",
    "start_year = 2016\n",
    "\n",
    "for i in range(0, 3):\n",
    "    num_affected_zcta = px.find_num_people_affected_by_geo(\n",
    "        path_to_hazards=wildfire_paths[i],\n",
    "        path_to_additional_geos=zcta_path,\n",
    "        raster_path=ghsl_path,\n",
    "        by_unique_hazard=False # setting by unique hazard to false \n",
    "    )\n",
    "    num_affected_zcta['year'] = start_year + i\n",
    "    num_affected_zcta_list.append(num_affected_zcta)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the case where we were not computing by ZCTA, the results come with \n",
    "hazard ids for groups of overlapping hazards.\n",
    "\n",
    "Since we're interested in the total number of people affected by ZCTA, we'll \n",
    "group the output dataframe by ZCTA and year and sum over any hazard IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# putting all years into one dataframe\n",
    "num_affected_zcta_df = pd.concat(num_affected_zcta_list, axis=0)\n",
    "num_affected_zcta_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_num_af = num_affected_zcta_df.groupby([\"ID_spatial_unit\", \"year\"]).agg(\n",
    "     {\"num_people_affected\": \"sum\"}\n",
    ").reset_index()\n",
    "\n",
    "# And we can save \n",
    "agg_num_af.to_parquet(data_dir / \"03_results\" / \"num_people_affected_by_wildfire_by_zcta.parquet\")\n",
    "agg_num_af.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our final case of counting exposed people, in which we aim to find the number of people affected by each hazard by each ZCTA, we do the same as we just did, but with by_unique_hazard = True. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_affected_zcta_unique_list = []\n",
    "start_year = 2016\n",
    "\n",
    "for i in range(0, 3):\n",
    "    num_affected_zcta_unique = px.find_num_people_affected_by_geo(\n",
    "        path_to_hazards=wildfire_paths[i],\n",
    "        path_to_additional_geos=zcta_path,\n",
    "        raster_path=ghsl_path,\n",
    "        by_unique_hazard=True # setting by unique hazard to true \n",
    "    )\n",
    "    num_affected_zcta_unique['year'] = start_year + i\n",
    "    num_affected_zcta_unique_list.append(num_affected_zcta_unique)\n",
    "\n",
    "# all years in one dataframe\n",
    "num_affected_df_zcta_unique = pd.concat(num_affected_zcta_unique_list, axis=0)\n",
    "\n",
    "# and we can save\n",
    "num_affected_df_zcta_unique.to_parquet(data_dir / \"03_results\" / \"num_people_affected_by_wildfire_zcta_unique.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will explore some of the output from these runs in the next section of the tutorial.  \n",
    "\n",
    "Finally, let's use the function find_number_of_people_residing_by_geo to get some denominators for our dataset. This function can help us use the gridded population data we used to find the number of people residing within the hazard buffers to also find the number of people residing in each ZCTA. This is useful if we're using a gridded population dataset that we think is a big improvement over other population counts in our additional spatial units, or we just want to be consistent. \n",
    "\n",
    "To call this function, all we need to do is use the same paths we've used previously:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_residing_by_zcta = px.find_number_of_people_residing_by_geo(\n",
    "    path_to_additional_geos=zcta_path,\n",
    "    raster_path=ghsl_path\n",
    ")\n",
    "\n",
    "num_residing_by_zcta.to_parquet(data_dir / \"03_results\" / \"num_people_residing_by_zcta.parquet\")\n",
    "num_residing_by_zcta.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, we have a column for spatial unit and a column for the number of people living in that spatial unit. \n",
    "Please continue to part 3 of this tutorial to explore the output of these functions! \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
